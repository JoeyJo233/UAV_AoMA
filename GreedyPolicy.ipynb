{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abf9046d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始运行固定10000步的贪心策略...\n",
      "Episode    1 | Reward:    5585.34 | Avg AoMA:   211.11 ( 50033.0 / 237) | Early Stop: ✅ | Stopped at: 10014\n",
      "Episode    2 | Reward:    3987.90 | Avg AoMA:   253.22 ( 61280.0 / 242) | Early Stop: ✅ | Stopped at: 10001\n",
      "Episode    3 | Reward:    4320.05 | Avg AoMA:   242.91 ( 55869.0 / 230) | Early Stop: ✅ | Stopped at: 10003\n",
      "Episode    4 | Reward:    4924.49 | Avg AoMA:   229.78 ( 52390.0 / 228) | Early Stop: ✅ | Stopped at: 10004\n",
      "Episode    5 | Reward:    4745.53 | Avg AoMA:   234.44 ( 50640.0 / 216) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode    6 | Reward:    4039.07 | Avg AoMA:   253.54 ( 56793.0 / 224) | Early Stop: ✅ | Stopped at: 10012\n",
      "Episode    7 | Reward:    5109.94 | Avg AoMA:   225.64 ( 46031.0 / 204) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode    8 | Reward:    3683.73 | Avg AoMA:   266.67 ( 57067.0 / 214) | Early Stop: ✅ | Stopped at: 10003\n",
      "Episode    9 | Reward:    5192.19 | Avg AoMA:   216.11 ( 50354.0 / 233) | Early Stop: ✅ | Stopped at: 10005\n",
      "Episode   10 | Reward:    4678.11 | Avg AoMA:   232.16 ( 51771.0 / 223) | Early Stop: ✅ | Stopped at: 10003\n",
      "Episode   11 | Reward:    4758.97 | Avg AoMA:   233.21 ( 48975.0 / 210) | Early Stop: ✅ | Stopped at: 10009\n",
      "Episode   12 | Reward:    4411.87 | Avg AoMA:   246.55 ( 58925.0 / 239) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   13 | Reward:    5010.82 | Avg AoMA:   227.14 ( 52697.0 / 232) | Early Stop: ✅ | Stopped at: 10012\n",
      "Episode   14 | Reward:    4429.56 | Avg AoMA:   240.46 ( 53141.0 / 221) | Early Stop: ✅ | Stopped at: 10011\n",
      "Episode   15 | Reward:    5015.67 | Avg AoMA:   224.96 ( 49717.0 / 221) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   16 | Reward:    4827.88 | Avg AoMA:   231.88 ( 55187.0 / 238) | Early Stop: ✅ | Stopped at: 10008\n",
      "Episode   17 | Reward:    4137.27 | Avg AoMA:   252.39 ( 57544.0 / 228) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   18 | Reward:    4095.87 | Avg AoMA:   246.31 ( 53202.0 / 216) | Early Stop: ✅ | Stopped at: 10009\n",
      "Episode   19 | Reward:    4129.23 | Avg AoMA:   253.54 ( 57299.0 / 226) | Early Stop: ✅ | Stopped at: 10013\n",
      "Episode   20 | Reward:    4052.20 | Avg AoMA:   256.96 ( 60129.0 / 234) | Early Stop: ✅ | Stopped at: 10004\n",
      "Episode   21 | Reward:    4882.00 | Avg AoMA:   230.33 ( 49751.0 / 216) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   22 | Reward:    3762.58 | Avg AoMA:   261.77 ( 58899.0 / 225) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   23 | Reward:    4228.67 | Avg AoMA:   250.16 ( 59539.0 / 238) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   24 | Reward:    4628.21 | Avg AoMA:   239.21 ( 56454.0 / 236) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   25 | Reward:    4698.13 | Avg AoMA:   236.52 ( 56056.0 / 237) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   26 | Reward:    4653.32 | Avg AoMA:   240.67 ( 54151.0 / 225) | Early Stop: ✅ | Stopped at: 10011\n",
      "Episode   27 | Reward:    5681.80 | Avg AoMA:   213.56 ( 50828.0 / 238) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   28 | Reward:    4347.01 | Avg AoMA:   246.64 ( 55493.0 / 225) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   29 | Reward:    4157.91 | Avg AoMA:   249.10 ( 53806.0 / 216) | Early Stop: ✅ | Stopped at: 10009\n",
      "Episode   30 | Reward:    4138.86 | Avg AoMA:   251.19 ( 54509.0 / 217) | Early Stop: ✅ | Stopped at: 10002\n",
      "Episode   31 | Reward:    5045.77 | Avg AoMA:   230.10 ( 52233.0 / 227) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   32 | Reward:    4888.05 | Avg AoMA:   231.79 ( 54008.0 / 233) | Early Stop: ✅ | Stopped at: 10005\n",
      "Episode   33 | Reward:    5574.68 | Avg AoMA:   212.58 ( 44855.0 / 211) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   34 | Reward:    4887.99 | Avg AoMA:   227.58 ( 50750.0 / 223) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   35 | Reward:    5012.50 | Avg AoMA:   222.93 ( 50159.0 / 225) | Early Stop: ✅ | Stopped at: 10011\n",
      "Episode   36 | Reward:    4752.66 | Avg AoMA:   232.79 ( 51213.0 / 220) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   37 | Reward:    3532.71 | Avg AoMA:   269.62 ( 63900.0 / 237) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   38 | Reward:    4745.41 | Avg AoMA:   237.64 ( 53232.0 / 224) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   39 | Reward:    4812.47 | Avg AoMA:   233.07 ( 53139.0 / 228) | Early Stop: ✅ | Stopped at: 10006\n",
      "Episode   40 | Reward:    4437.51 | Avg AoMA:   250.55 ( 59129.0 / 236) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   41 | Reward:    4119.04 | Avg AoMA:   245.05 ( 58078.0 / 237) | Early Stop: ✅ | Stopped at: 10014\n",
      "Episode   42 | Reward:    4584.43 | Avg AoMA:   239.45 ( 52679.0 / 220) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   43 | Reward:    5048.05 | Avg AoMA:   224.14 ( 49536.0 / 221) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   44 | Reward:    4697.94 | Avg AoMA:   235.64 ( 53961.0 / 229) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   45 | Reward:    4503.65 | Avg AoMA:   242.06 ( 55916.0 / 231) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   46 | Reward:    5795.03 | Avg AoMA:   205.59 ( 45847.0 / 223) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   47 | Reward:    4582.41 | Avg AoMA:   240.41 ( 56256.0 / 234) | Early Stop: ✅ | Stopped at: 10008\n",
      "Episode   48 | Reward:    3347.30 | Avg AoMA:   273.18 ( 64471.0 / 236) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   49 | Reward:    4987.86 | Avg AoMA:   225.10 ( 49523.0 / 220) | Early Stop: ✅ | Stopped at: 10006\n",
      "Episode   50 | Reward:    3967.01 | Avg AoMA:   255.79 ( 57808.0 / 226) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   51 | Reward:    4736.60 | Avg AoMA:   241.00 ( 53743.0 / 223) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   52 | Reward:    4382.17 | Avg AoMA:   240.39 ( 50962.0 / 212) | Early Stop: ✅ | Stopped at: 10006\n",
      "Episode   53 | Reward:    4128.52 | Avg AoMA:   254.61 ( 53978.0 / 212) | Early Stop: ✅ | Stopped at: 10009\n",
      "Episode   54 | Reward:    4162.91 | Avg AoMA:   245.80 ( 55551.0 / 226) | Early Stop: ✅ | Stopped at: 10011\n",
      "Episode   55 | Reward:    4238.34 | Avg AoMA:   247.06 ( 53613.0 / 217) | Early Stop: ✅ | Stopped at: 10001\n",
      "Episode   56 | Reward:    3845.08 | Avg AoMA:   260.80 ( 61028.0 / 234) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   57 | Reward:    4928.03 | Avg AoMA:   225.93 ( 51738.0 / 229) | Early Stop: ✅ | Stopped at: 10007\n",
      "Episode   58 | Reward:    4255.03 | Avg AoMA:   249.15 ( 57304.0 / 230) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   59 | Reward:    5306.41 | Avg AoMA:   218.28 ( 48239.0 / 221) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   60 | Reward:    4411.78 | Avg AoMA:   246.97 ( 58285.0 / 236) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   61 | Reward:    4338.12 | Avg AoMA:   247.96 ( 53808.0 / 217) | Early Stop: ✅ | Stopped at: 10008\n",
      "Episode   62 | Reward:    5666.65 | Avg AoMA:   204.58 ( 46439.0 / 227) | Early Stop: ✅ | Stopped at: 10004\n",
      "Episode   63 | Reward:    4652.11 | Avg AoMA:   235.83 ( 50467.0 / 214) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   64 | Reward:    5331.71 | Avg AoMA:   217.44 ( 47185.0 / 217) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   65 | Reward:    5552.53 | Avg AoMA:   219.29 ( 53287.0 / 243) | Early Stop: ✅ | Stopped at: 10009\n",
      "Episode   66 | Reward:    3291.22 | Avg AoMA:   274.50 ( 60391.0 / 220) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   67 | Reward:    4879.75 | Avg AoMA:   228.29 ( 52736.0 / 231) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   68 | Reward:    4284.63 | Avg AoMA:   237.39 ( 53413.0 / 225) | Early Stop: ✅ | Stopped at: 10018\n",
      "Episode   69 | Reward:    4999.01 | Avg AoMA:   229.62 ( 54650.0 / 238) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   70 | Reward:    3725.69 | Avg AoMA:   263.64 ( 61691.0 / 234) | Early Stop: ✅ | Stopped at: 10005\n",
      "Episode   71 | Reward:    4754.96 | Avg AoMA:   234.72 ( 55394.0 / 236) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   72 | Reward:    4980.70 | Avg AoMA:   226.09 ( 49062.0 / 217) | Early Stop: ✅ | Stopped at: 10010\n",
      "Episode   73 | Reward:    4809.17 | Avg AoMA:   232.07 ( 53375.0 / 230) | Early Stop: ✅ | Stopped at: 10005\n",
      "Episode   74 | Reward:    4943.75 | Avg AoMA:   233.22 ( 52942.0 / 227) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   75 | Reward:    5178.93 | Avg AoMA:   221.36 ( 49141.0 / 222) | Early Stop: ✅ | Stopped at: 10001\n",
      "Episode   76 | Reward:    4251.72 | Avg AoMA:   249.07 ( 54048.0 / 217) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   77 | Reward:    5505.55 | Avg AoMA:   210.81 ( 43848.0 / 208) | Early Stop: ✅ | Stopped at: 10002\n",
      "Episode   78 | Reward:    4345.64 | Avg AoMA:   249.73 ( 56689.0 / 227) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   79 | Reward:    5264.29 | Avg AoMA:   223.44 ( 49604.0 / 222) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   80 | Reward:    3840.70 | Avg AoMA:   252.89 ( 57659.0 / 228) | Early Stop: ✅ | Stopped at: 10014\n",
      "Episode   81 | Reward:    4506.29 | Avg AoMA:   239.39 ( 59129.0 / 247) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   82 | Reward:    4768.54 | Avg AoMA:   231.05 ( 53142.0 / 230) | Early Stop: ✅ | Stopped at: 10016\n",
      "Episode   83 | Reward:    5811.31 | Avg AoMA:   202.91 ( 42206.0 / 208) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   84 | Reward:    4885.91 | Avg AoMA:   233.30 ( 55059.0 / 236) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   85 | Reward:    4413.36 | Avg AoMA:   244.54 ( 56734.0 / 232) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   86 | Reward:    4397.29 | Avg AoMA:   241.66 ( 55341.0 / 229) | Early Stop: ✅ | Stopped at: 10013\n",
      "Episode   87 | Reward:    4837.26 | Avg AoMA:   235.80 ( 55412.0 / 235) | Early Stop: ✅ | Stopped at: 10013\n",
      "Episode   88 | Reward:    4748.29 | Avg AoMA:   234.93 ( 54268.0 / 231) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   89 | Reward:    4397.68 | Avg AoMA:   244.04 ( 56129.0 / 230) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   90 | Reward:    4972.92 | Avg AoMA:   229.98 ( 54046.0 / 235) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   91 | Reward:    4685.90 | Avg AoMA:   236.39 ( 55788.0 / 236) | Early Stop: ✅ | Stopped at: 10013\n",
      "Episode   92 | Reward:    5569.50 | Avg AoMA:   213.07 ( 48794.0 / 229) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   93 | Reward:    4343.93 | Avg AoMA:   249.06 ( 57284.0 / 230) | Early Stop: ✅ | Stopped at: 10003\n",
      "Episode   94 | Reward:    5388.07 | Avg AoMA:   222.01 ( 51729.0 / 233) | Early Stop: ✅ | Stopped at: 10004\n",
      "Episode   95 | Reward:    4591.32 | Avg AoMA:   237.47 ( 50581.0 / 213) | Early Stop: ✅ | Stopped at: 10011\n",
      "Episode   96 | Reward:    5728.50 | Avg AoMA:   214.46 ( 49325.0 / 230) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode   97 | Reward:    4974.80 | Avg AoMA:   231.49 ( 56253.0 / 243) | Early Stop: ✅ | Stopped at: 10017\n",
      "Episode   98 | Reward:    5560.79 | Avg AoMA:   214.46 ( 50397.0 / 235) | Early Stop: ✅ | Stopped at: 10001\n",
      "Episode   99 | Reward:    4876.17 | Avg AoMA:   230.12 ( 49476.0 / 215) | Early Stop: ✅ | Stopped at: 10000\n",
      "Episode  100 | Reward:    4051.80 | Avg AoMA:   255.23 ( 54875.0 / 215) | Early Stop: ✅ | Stopped at: 10007\n",
      "Episode  100 | Avg Reward:  4661.38 | Avg AoMA: 236.97 | Anchors:  0.0 | Violations:  0.0\n",
      "平均AoMA: 236.97\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ---------------------------\n",
    "# 环境定义（固定10000步版本）\n",
    "# ---------------------------\n",
    "class UAVEnv:\n",
    "    def __init__(self):\n",
    "        # 地点表示: 0: L0 (Home), 1: L1, 2: L2\n",
    "        # 物理参数\n",
    "        self.d_L0_L1 = 10\n",
    "        self.d_L0_L2 = 20\n",
    "        self.d_L1_L2 = 15  # 假设 L1 到 L2距离（对称）\n",
    "        self.charging_rate = 20  # 充电速率\n",
    "        self.remove_cost = 5     # 消除警报消耗能量（可调）\n",
    "        self.battery_capacity = 80\n",
    "        \n",
    "        # 警报生成概率（独立变量）\n",
    "        self.p_L1 = 0.017\n",
    "        self.p_L2 = 0.012\n",
    "\n",
    "        # 调整惩罚和奖励参数\n",
    "        self.max_alarm_age = 100  # 最大等待时间\n",
    "        self.anchor_penalty = 50  # 降低抛锚惩罚（不再直接终止）\n",
    "        self.max_age_penalty = 50\n",
    "        self.clear_reward_base = 10\n",
    "\n",
    "        # 固定步数为10000\n",
    "        self.max_steps = 10000      \n",
    "        self.alarm_penalty_rate = 2\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # 当前时间步\n",
    "        self.t = 0\n",
    "        # UAV位置，初始在L0(Home)\n",
    "        self.pos = 0\n",
    "        # 充满电\n",
    "        self.battery = self.battery_capacity\n",
    "        \n",
    "        # 对每个地点的警报状态：如果有警报，记录等待时间（初始值为1）\n",
    "        # 如果没有警报，记为0\n",
    "        self.alarm_L1 = 0  \n",
    "        self.alarm_L2 = 0  \n",
    "        \n",
    "        # 累计 AoMA\n",
    "        self.aoa_total = 0\n",
    "        # 性能统计\n",
    "        self.alarm_count = 0\n",
    "        self.anchor_count = 0  # 抛锚次数\n",
    "        self.max_age_violations = 0  # 超时次数\n",
    "        self.alarms_cleared = 0  # 成功清除的警报数\n",
    "        \n",
    "        # 新增状态属性\n",
    "        self.stop_reason = \"NO\"\n",
    "        \n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        # 状态: [当前所在位置(one-hot 3维), 剩余电量归一化, alarm_L1等待时间归一化, alarm_L2等待时间归一化]\n",
    "        pos_onehot = [1 if self.pos == i else 0 for i in range(3)]\n",
    "        norm_battery = self.battery / self.battery_capacity\n",
    "        # 用最大允许等待时长做归一化并截断到 [0,1]\n",
    "        norm_alarm1 = min(self.alarm_L1, self.max_alarm_age) / self.max_alarm_age\n",
    "        norm_alarm2 = min(self.alarm_L2, self.max_alarm_age) / self.max_alarm_age\n",
    "        state = np.array(pos_onehot + [norm_battery, norm_alarm1, norm_alarm2], dtype=np.float32)\n",
    "        return state\n",
    "\n",
    "    def _get_distance(self, from_pos, to_pos):\n",
    "        if from_pos == to_pos:\n",
    "            return 0\n",
    "        if {from_pos, to_pos} == {0, 1}:\n",
    "            return self.d_L0_L1\n",
    "        if {from_pos, to_pos} == {0, 2}:\n",
    "            return self.d_L0_L2\n",
    "        if {from_pos, to_pos} == {1, 2}:\n",
    "            return self.d_L1_L2\n",
    "        return 0\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        动作定义:\n",
    "         0: 移动到 L0 (如果已在L0，则代表在Home充电)\n",
    "         1: 移动到 L1\n",
    "         2: 移动到 L2\n",
    "         3: 执行消除警报动作（前提是在对应地点且有警报）\n",
    "        \"\"\"\n",
    "        reward = 0.0\n",
    "        delta_time = 1\n",
    "\n",
    "        # 处理动作\n",
    "        if action in [0, 1, 2]:\n",
    "            target = action  # 目标地点\n",
    "            if self.pos != target:\n",
    "                # 计算移动需要的耗时和能量\n",
    "                dist = self._get_distance(self.pos, target)\n",
    "                # 如果电量不足以完成移动，则给予惩罚但不终止episode\n",
    "                if self.battery < dist:\n",
    "                    self.anchor_count += 1\n",
    "                    self.stop_reason = \"Anchored\"\n",
    "                    reward -= self.anchor_penalty\n",
    "                    # 强制充电：瞬间回到L0并充满电\n",
    "                    self.pos = 0\n",
    "                    self.battery = self.battery_capacity\n",
    "                    delta_time = dist  # 保持时间消耗\n",
    "                else:\n",
    "                    # 正常飞行\n",
    "                    self.battery -= dist\n",
    "                    delta_time = dist\n",
    "                    self.pos = target\n",
    "            \n",
    "            # 如果在L0，则可以充电\n",
    "            if self.pos == 0:\n",
    "                charge = self.charging_rate * delta_time\n",
    "                self.battery = min(self.battery + charge, self.battery_capacity)\n",
    "                \n",
    "        elif action == 3:\n",
    "            # 执行消除警报动作\n",
    "            if self.pos == 1 and self.alarm_L1 > 0:\n",
    "                if self.battery < self.remove_cost:\n",
    "                    self.anchor_count += 1\n",
    "                    self.stop_reason = \"Anchored\"\n",
    "                    reward -= self.anchor_penalty\n",
    "                    # 强制充电\n",
    "                    self.pos = 0\n",
    "                    self.battery = self.battery_capacity\n",
    "                else:\n",
    "                    self.battery -= self.remove_cost\n",
    "                    t = self.alarm_L1\n",
    "                    self.aoa_total += t * (t + 1) / 2\n",
    "                    time_bonus = max(-5, 60 - (self.alarm_L1+1)*(self.alarm_L1+2)/40)\n",
    "                    reward += time_bonus\n",
    "                    self.alarm_L1 = 0\n",
    "                    self.alarms_cleared += 1\n",
    "                    \n",
    "            elif self.pos == 2 and self.alarm_L2 > 0:\n",
    "                if self.battery < self.remove_cost:\n",
    "                    self.anchor_count += 1\n",
    "                    self.stop_reason = \"Anchored\"\n",
    "                    reward -= self.anchor_penalty\n",
    "                    # 强制充电\n",
    "                    self.pos = 0\n",
    "                    self.battery = self.battery_capacity\n",
    "                else:\n",
    "                    self.battery -= self.remove_cost\n",
    "                    t = self.alarm_L2\n",
    "                    self.aoa_total += t * (t + 1) / 2\n",
    "                    time_bonus = max(-5, 60 - (self.alarm_L2+1)*(self.alarm_L2+2)/40)\n",
    "                    reward += time_bonus\n",
    "                    self.alarm_L2 = 0\n",
    "                    self.alarms_cleared += 1\n",
    "            else:\n",
    "                # 无效的消除动作\n",
    "                reward -= 20\n",
    "        else:\n",
    "            # 非法动作\n",
    "            reward -= 20\n",
    "\n",
    "        # 时间步进\n",
    "        for _ in range(delta_time):\n",
    "            self.t += 1\n",
    "            \n",
    "            # 更新现有警报的等待时间\n",
    "            if self.alarm_L1 > 0:\n",
    "                self.alarm_L1 += 1\n",
    "            if self.alarm_L2 > 0:\n",
    "                self.alarm_L2 += 1\n",
    "            \n",
    "            # 持续惩罚\n",
    "            alarm_penalty = 0\n",
    "            if self.alarm_L1 > 0:\n",
    "                alarm_penalty += self.alarm_penalty_rate*(0.8+ 1.5*self.alarm_L1/self.max_alarm_age)\n",
    "            if self.alarm_L2 > 0:\n",
    "                alarm_penalty += self.alarm_penalty_rate*(0.8+ 1.5*self.alarm_L2/self.max_alarm_age)\n",
    "            reward -= alarm_penalty\n",
    "            \n",
    "            # 检查警报是否超过最大等待时间（给予惩罚但不终止）\n",
    "            if self.alarm_L1 > self.max_alarm_age:\n",
    "                self.max_age_violations += 1\n",
    "                reward -= self.max_age_penalty\n",
    "                # 重置超时警报\n",
    "                self.aoa_total += self.alarm_L1 * (self.alarm_L1 + 1) / 2\n",
    "                self.alarm_L1 = 0\n",
    "                \n",
    "            if self.alarm_L2 > self.max_alarm_age:\n",
    "                self.max_age_violations += 1\n",
    "                reward -= self.max_age_penalty\n",
    "                # 重置超时警报\n",
    "                self.aoa_total += self.alarm_L2 * (self.alarm_L2 + 1) / 2\n",
    "                self.alarm_L2 = 0\n",
    "            \n",
    "            # 尝试生成新的警报\n",
    "            if self.alarm_L1 == 0 and random.random() < self.p_L1:\n",
    "                self.alarm_L1 = 1\n",
    "                self.alarm_count += 1\n",
    "            if self.alarm_L2 == 0 and random.random() < self.p_L2:\n",
    "                self.alarm_L2 = 1\n",
    "                self.alarm_count += 1\n",
    "\n",
    "        # 电量安全奖励\n",
    "        if self.battery > 40:\n",
    "            reward += 0.5\n",
    "        elif self.battery < 20:\n",
    "            reward -= (self.battery_capacity - self.battery) + 60\n",
    "\n",
    "        # 固定在10000步结束\n",
    "        done = False\n",
    "        if self.t >= self.max_steps or self.battery <= 0:\n",
    "            done = True\n",
    "            self.stop_reason = \"✅\"\n",
    "            if self.battery <= 0:\n",
    "                self.anchored = True\n",
    "                self.stop_reason = \"Anchored\"\n",
    "                reward -= self.anchor_penalty\n",
    "\n",
    "        return self._get_state(), reward, done, {}\n",
    "# ---------------------------\n",
    "# 贪心策略函数（修正版）\n",
    "# ---------------------------\n",
    "def select_action_greedy(env):\n",
    "    \"\"\"\n",
    "    改进的贪心策略：\n",
    "    1. 考虑无人机当前位置计算准确的能耗\n",
    "    2. 优先处理年龄更大的警报\n",
    "    3. 如果电量不够完成任务+返回L0，则先回L0充电\n",
    "    4. 无警报时待在L0充电\n",
    "    \"\"\"\n",
    "    alarm_at_L1 = (env.alarm_L1 > 0)\n",
    "    alarm_at_L2 = (env.alarm_L2 > 0)\n",
    "    \n",
    "    # 如果没有警报，回L0充电\n",
    "    if not alarm_at_L1 and not alarm_at_L2:\n",
    "        return 0\n",
    "    \n",
    "    # 计算处理每个警报的成本（基于当前位置）\n",
    "    tasks = []\n",
    "    \n",
    "    if alarm_at_L1:\n",
    "        cost_to_reach_L1 = env._get_distance(env.pos, 1)\n",
    "        cost_to_clear = env.remove_cost\n",
    "        cost_to_return_from_L1 = env._get_distance(1, 0)\n",
    "        total_cost_L1 = cost_to_reach_L1 + cost_to_clear + cost_to_return_from_L1\n",
    "        \n",
    "        tasks.append({\n",
    "            'location': 1,\n",
    "            'alarm_age': env.alarm_L1,\n",
    "            'total_cost': total_cost_L1,\n",
    "            'cost_to_reach': cost_to_reach_L1,\n",
    "            'cost_to_clear_and_return': cost_to_clear + cost_to_return_from_L1\n",
    "        })\n",
    "    \n",
    "    if alarm_at_L2:\n",
    "        cost_to_reach_L2 = env._get_distance(env.pos, 2)\n",
    "        cost_to_clear = env.remove_cost\n",
    "        cost_to_return_from_L2 = env._get_distance(2, 0)\n",
    "        total_cost_L2 = cost_to_reach_L2 + cost_to_clear + cost_to_return_from_L2\n",
    "        \n",
    "        tasks.append({\n",
    "            'location': 2,\n",
    "            'alarm_age': env.alarm_L2,\n",
    "            'total_cost': total_cost_L2,\n",
    "            'cost_to_reach': cost_to_reach_L2,\n",
    "            'cost_to_clear_and_return': cost_to_clear + cost_to_return_from_L2\n",
    "        })\n",
    "    \n",
    "    # 按警报年龄排序（年龄大的优先）\n",
    "    tasks.sort(key=lambda x: x['alarm_age'], reverse=True)\n",
    "    \n",
    "    # 尝试执行优先级最高的任务\n",
    "    for task in tasks:\n",
    "        target_loc = task['location']\n",
    "        \n",
    "        if env.pos == target_loc:\n",
    "            # 已在目标位置，检查是否有足够电量消除警报并返回\n",
    "            if env.battery >= task['cost_to_clear_and_return']:\n",
    "                return 3  # 消除警报\n",
    "            else:\n",
    "                return 0  # 电量不足，回L0充电\n",
    "        else:\n",
    "            # 不在目标位置，检查是否有足够电量完成整个任务\n",
    "            if env.battery >= task['total_cost']:\n",
    "                return target_loc  # 前往目标位置\n",
    "            # 如果电量不足，继续检查下一个优先级的任务\n",
    "    \n",
    "    # 所有任务都无法完成，回L0充电\n",
    "    return 0\n",
    "\n",
    "# ---------------------------\n",
    "# 训练函数（使用贪心策略）\n",
    "# ---------------------------\n",
    "def train_greedy(num_episodes=1000):\n",
    "    env = UAVEnv()\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_aoa = []\n",
    "    episode_stats = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        # 固定执行10000步\n",
    "        while not done:\n",
    "            action = select_action_greedy(env)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        # 计算平均 AoMA\n",
    "        avg_aoa = env.aoa_total / env.alarm_count if env.alarm_count > 0 else 0\n",
    "        \n",
    "        # 新增：按需求格式打印\n",
    "        print(\n",
    "            f\"Episode {ep+1:4d} | Reward: {total_reward:10.2f}\"\n",
    "            f\" | Avg AoMA: {avg_aoa:8.2f} ({env.aoa_total:8} / {env.alarm_count:2d})\"\n",
    "            f\" | Early Stop: {env.stop_reason}\"\n",
    "            f\" | Stopped at: {env.t}\"\n",
    "        )\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_aoa.append(avg_aoa)\n",
    "        \n",
    "        # 收集详细统计\n",
    "        episode_stats.append({\n",
    "            'total_alarms': env.alarm_count,\n",
    "            'cleared_alarms': env.alarms_cleared,\n",
    "            'anchor_count': env.anchor_count,\n",
    "            'max_age_violations': env.max_age_violations,\n",
    "            'avg_aoa': avg_aoa,\n",
    "            'total_reward': total_reward\n",
    "        })\n",
    "    \n",
    "        # 每100集打印一次信息\n",
    "        if (ep + 1) % 100 == 0:\n",
    "            recent_stats = episode_stats[-100:]\n",
    "            avg_reward = np.mean([s['total_reward'] for s in recent_stats])\n",
    "            avg_anchors = np.mean([s['anchor_count'] for s in recent_stats])\n",
    "            avg_violations = np.mean([s['max_age_violations'] for s in recent_stats])\n",
    "            avg_aoa_recent = np.mean([s['avg_aoa'] for s in recent_stats])\n",
    "            \n",
    "            print(f\"Episode {ep+1:4d} | \"\n",
    "                  f\"Avg Reward: {avg_reward:8.2f} | \"\n",
    "                  f\"Avg AoMA: {avg_aoa_recent:6.2f} | \"\n",
    "                  f\"Anchors: {avg_anchors:4.1f} | \"\n",
    "                  f\"Violations: {avg_violations:4.1f}\")\n",
    "    \n",
    "    return episode_rewards, episode_aoa, episode_stats\n",
    "\n",
    "print(\"开始运行固定10000步的贪心策略...\")\n",
    "greedy_rewards, greedy_aoa, greedy_stats = train_greedy(num_episodes=100)\n",
    "\n",
    "# 打印总体平均 AoMA\n",
    "import numpy as np\n",
    "print(f\"平均AoMA: {np.mean(greedy_aoa):.2f}\")\n",
    "\n",
    "# # 绘制贪心策略的固定步数性能\n",
    "# print(\"\\n生成贪心策略固定10000步性能图表...\")\n",
    "# greedy_performance = plot_fixed_step_performance(greedy_rewards, greedy_aoa, greedy_stats, \n",
    "#                                                 \"贪心策略 - 固定10000步性能分析\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UAV1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
